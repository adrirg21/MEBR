---
title: "Métodos Estadísticos con R"
subtitle: "CODD"
author: "Gibrán Peniche"
date: "(versión 0.0.1) - 2020/06/25"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: ["xaringan-themer.css", "style.css"]
    nature:
      highlightStyle: github
      highlightLines: TRUE
      countIncrementalSlides: TRUE
      self_contained: TRUE
      ratio: '16:9'
    seal: false
editor_options: 
  chunk_output_type: console
---


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(eval = TRUE)
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(fig.align = 'center')
library(tidyverse)
library(gt)
library(patchwork)
library(tidybayes)
library(latex2exp)

```

class: title-slide, middle

.pull-left[ 
# Métodos Estadísticos Bayesianos con R
## Contraste de Hipótesis (Compuesta vs. Compuesta)
### Gibrán Peniche
### v. 0.0.1
### 2020-10-29
####  <i class="fab fa-github"></i> [jgpeniche](https://github.com/jgpeniche)
####  <i class="fab fa-twitter"></i> [PenicheGibran](https://twitter.com/PenicheGibran)
####  <i class="fab fa-google"></i> jgpeniche@gmail.com

]

.pull-right[

![](figs/logo_g.png)

]

---

# La sesión pasada...

--

- Estudiamos el contexto histórico y las dos propuestas para el contraste de hipótesis que se plantearon en la escuela de la Estadística Matemática

--

- Recordamos la solución al problema de contraste de hipótesis propuesto por Neyman y Pearson

--

- Resolvimos el problema de hipótesis simple contra simple desde el punto de vista Bayesiano

---

# Agenda

  1. Resolver el problema de contraste de hipótesis compuesta contra compuesta
  
---

class: inverse, center, middle

# 1

--

# Contraste de Hipótesis (Compuesta contra Compuesta)

---

# Contraste de Hipótesis (Compuesta contra Compuesta)

--

Consideremos un $\underline{X}_{(n)}$ con $X$ con f.d.p.g $f(x, \theta)$ con $\theta$ $\epsilon$ $\Theta$ $\subseteq$ $\mathbb{R}^k$.

--

Nos preguntamos por el juego de hipótesis $$H_0: \theta \quad \epsilon \quad A \quad \subseteq \quad \Theta \quad vs \quad H_1: \theta \quad \epsilon \quad A^c \quad \subseteq \quad \Theta$$
--

Nos dimos cuenta que el problema de Contraste de Hipótesis es equivalente a un problema de decisión $$(\prec, \mathbb{E}, \mathbb{C}, \mathfrak{C})$$

--

Equivalente a $\mathbb{D} = \{d_i, d_j \}$ tal que $d_k = H_k: f(x,\theta) =f(x, \theta \epsilon A^k)$

--

Además si definimos una función de pérdida $\mathcal{L}(c(d_k, \theta)) = l_0(\theta)$

---

# Contraste de Hipótesis (Compuesta contra Compuesta)

--

$$\mathbb{E}[\mathfrak{L}_i] < \mathbb{E}[\mathfrak{L}_k]$$

--

En el caso **continuo** forzozamente la pérdida esperada *a posteriori* queda definida como $$\mathbb{E}[\mathcal{L}(d_k,\theta)] = \int_{\Theta} l_k(\theta) P(\theta | \underline{X}_{(n)}) \partial \theta$$

--

En consecuencia la regla decisión del cociente de pérdidas esperadas está dada por $$\Lambda = \frac{\int_{\Theta} l_i(\theta) P(\theta | \underline{X}_{(n)}) \partial \theta}{\int_{\Theta} l_j(\theta) P(\theta | \underline{X}_{(n)}) \partial \theta} < 1$$

--

A diferencia del cociente de verosimilitudes genralizado $\Lambda = \frac{\sup_{H_i} \mathbb{L}[\theta_i|\underline{X}_{(n)}]}{sup_{H_i \cup H_j} \mathbb{L}[\theta_{MV} | \underline{X}_{(n)}]}$, las verosimilitudes bayesianas están penalizadas por $l_i(\theta)$ con respecto de la medida de probabilidad $P(\theta)$ y no con respecto de los supremos.

---

# Contraste de Hipótesis (Compuesta contra Compuesta)

Si además. al igual que en casi simple contra simple, simplificamos la función pérdida tal que asigne pérdida 0 al acierto y una constante para el error:


$$l_k(\theta) = \left\{ \begin{array}{rcl} 0 & \mbox{si} & \theta \epsilon A^k \\ l_k & \mbox{si} & o.c \end{array}\right.$$

--

El cociente se simplifica y está dado por:

$$\Lambda = \frac{l_j \int_{A}  P(\theta | \underline{X}_{(n)}) \partial \theta}{l_i \int_{A^c} P(\theta | \underline{X}_{(n)}) \partial \theta} = \frac{l_j P(\theta \epsilon A)\int_{A}  P(\theta | \underline{X}_{(n)})P(\theta | \theta \epsilon A) \partial \theta}{l_i P(\theta \epsilon A^c) \int_{A^c} P(\theta | \underline{X}_{(n)})P(\theta | \theta \epsilon A^c) \partial \theta} = \frac{\mathbb{E}[L | H_0]}{  \mathbb{E}[L | H_1]} < 1$$
--

A esta contidad $\Lambda = BF$ tambipen se le conoce cómo **Factor de Bayes** y tiene la desventaja de que la posterior puede no tener forma análitica cerrada y requiere de *integración numérica* pero se puede generalizar para realizar el tipo de contraste $\mathcal{F}_i$ vs $\mathcal{F}_j$ dos familias paramétricas distintas. 

---

# Contraste de Hipótesis (Compuesta contra Compuesta)

A pesar de la discusión que acabamos de desarrollar, en sesiones pasadas concluimos que podíamos realizar pruebas de hipótesis a partir de nuestra *distribución posterior* y la distribución *posterior predictiva*. ¿Podemos hacer esa conjetura a partir de la misma discusión?

--

Ocurre que *a priori* $$\frac{l_j \int_{A} P(\theta) \partial \theta}{l_j \int_{A^c} P(\theta) \partial \theta} = \frac{l_i P(A)}{l_j P(A^c)}<1$$

--

Esto es equivalente a la regla *a posteriori* $\frac{l_i P(A | \underline{X}_{(n)})}{l_j P(A^c | \underline{X}_{(n)})} < 1$

--

Noten que $P(A | \underline{X}_{(n)}) = \int_{A} P(\theta | \underline{X}_{(n)}) \partial \theta = P(\theta \epsilon A | \underline{X}_{(n)})$ que es de hecho la probabilidad posterior de $\theta$ en una región. El caso que discutimos en clase fue el caso particular donde los errores producen la misma pérdida.

---

# Contraste de Hipótesis (Compuesta contra Compuesta)

Desde la perpspectiva frecuentista la expresión $P(\underline{X}_{(n)} | A)$ no tiene sentido en tanto que no determina un modelo p ara los datos que permita el cálculo de probabilidades sino una familia La relación entre la muestra 𝑥𝑛y los valores de 𝜃 se describe a través de un modelo de la forma $P(\underline{X}_{(n)} | \theta)$

--

Esta expresión equiere el valor puntual de 𝜃y aquí cobra especial relevancia la noción de que, para el análisis Bayesiano, el modelo de probabilidad no sólo incluye la componente que describe la variabilidad de los datos, sino también al que describe la incertidumbre sobre el parámetro fijo pero desconocido

---

# ¿Qué sigue?

--

1. Estadística Bayesiana Computacional

--

2. MCMC; Monte Carlo Markov Chain y todo su aparato teórico


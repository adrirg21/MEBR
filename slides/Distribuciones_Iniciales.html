<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Métodos Estadísticos con R</title>
    <meta charset="utf-8" />
    <meta name="author" content="Gibrán Peniche" />
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
    <link rel="stylesheet" href="style.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">





class: title-slide, middle

.pull-left[ 
# Métodos Estadísticos Bayesianos con R
## Distribuciones Iniciales y Funciones de Pérdida
### Gibrán Peniche
### v. 0.0.1
### 2020-07-21
####  &lt;i class="fab fa-github"&gt;&lt;/i&gt; [jgpeniche](https://github.com/jgpeniche)
####  &lt;i class="fab fa-twitter"&gt;&lt;/i&gt; [PenicheGibran](https://twitter.com/PenicheGibran)
####  &lt;i class="fab fa-google"&gt;&lt;/i&gt; jgpeniche@gmail.com

]

.pull-right[

![](figs/logo_g.png)

]

---
# La sesión pasada...

--

Migramos de *maximizar* la **utilidad** `\(u_0\)` a *minimizar* la **pérdida** `\(\mathcal{L}\)`, rescatando la noción de "distancia" del valor real `\(\theta\)`

--

A partir de los Axiomas de Coherencia dedujimos un algoritmo de 3 pasos para nuestro problema de inferencia:

--

1. Definir una función de pérdida `\(\mathcal{L}\)` y cuantificar la incertidumbre asociada a `\(\theta\)` con `\(f(\theta)\)` *a priori*

--
    
  1.1. En caso de existir información adicional (datos - verosimilitud) incorporarlos a través de Teorema de Bayes y obtener `\(P(\theta| X_{(n)})\)`
  
--

2. Minimzar la *pérdida esperada* `\(\int_{\Theta} \mathcal{L(\theta, \hat{\theta})}P(\theta| X_{(\underline{n})})\)` (en el caso **discreto**: `\(\sum_{\Theta} \mathcal{L(\theta, \hat{\theta})}P(\theta| X_{(\underline{n})})\)` )

--

3. Escoger `\(d_{\hat{\theta}}\)` que minimice dicha pérdida

---

# Agenda

--

1. Entender las repercusiones de distintas funciones de pérdida

--

  1.1. Pérdida Cuadrática
  
--

  1.2. Pérdida Absoluta
  
--

  1.3. Pérdida (0,1)
  
--

2. Distribuciones iniciales

---

class: inverse, center, middle
# 1

--

# Funciones de Pérdida

---

class: center, middle
--

# Pérdida Cuadrática

---

# Pérdida Cuadrática

--

Sea `\(\mathcal{L}(\hat{\theta}, \theta) = (\hat{\theta} - \theta)^2\)`

--

`\(\underset{\theta}{\arg\min} \quad \int_{\Theta} (\hat{\theta} - \theta)^2 P(\theta|X_{(\underline{n})})d \theta\)`

--

Pero `$$\int_{\Theta} (\hat{\theta} - \theta)^2 P(\theta|X_{(\underline{n})})d \theta = \hat{\theta}^2 \int_{\Theta} P(\theta|X_{(\underline{n})})d \theta \quad -2\hat{\theta} \int_{\Theta} \theta P(\theta|X_{(\underline{n})})d \theta \quad + \int_{\Theta} \theta^2 P(\theta|X_{(\underline{n})})d \theta$$`

--

Además sabemos que `\(\int_{\Theta} P(\theta|X_{(\underline{n})})d \theta = 1\)`

---

# Pérdida Cuadrática


Tomando la derivada con respecto de `\(\hat{\theta}\)` e igualando a 0

`$$\frac{\partial}{\partial \theta} = 2 \hat{\theta}  - 2 \int_{\Theta} \theta P(\theta|X_{(\underline{n})})d \theta = 0$$`

--

`\(\Longleftrightarrow\)`

`$$\hat{\theta} = \int_{\Theta} \theta P(\theta|X_{(\underline{n})})d \theta = \mathbb{E}[\theta|X_{(\underline{n})}]$$`

---

class: center, middle
# Pérdida Absoluta

---

# Pérdida Absoluta

--

Sea `\(\mathcal{L}(\hat{\theta}, \theta) = |\hat{\theta} - \theta|\)`

--

`\(\underset{\theta}{\arg\min} \quad \int_{\Theta} |\hat{\theta} - \theta| P(\theta|X_{(\underline{n})})d \theta\)`

--

Pero `$$\int_{\Theta} |\hat{\theta} - \theta| P(\theta|X_{(\underline{n})})d \theta =  \int_{-\infty}^{\theta} (\hat{\theta}-\theta)P(\theta|X_{(\underline{n})})d \theta \quad + \int_{\theta}^{\infty} (\theta - \hat{\theta})P(\theta|X_{(\underline{n})})d \theta$$`

---

# Pérdida Absoluta

Tomando la derivada con respecto a `\(\theta\)` e igualando a 0


`$$\int_{-\infty}^{\hat{\theta}} P(\theta|X_{(\underline{n})}) = \int_{\hat{\theta}}^{\infty} P(\theta|X_{(\underline{n})})$$`

--

`\(\Longleftrightarrow\)`

$$ \hat{\theta} = mediana$$
---

class: center, middle
# Pérdida (0,1)

---

# Pérdida (0,1)

--

Sea `$$\mathcal{L} =\left\{\begin{array}{ll} 1 &amp; \quad |\hat{\theta} - \theta| &gt; \epsilon \\ 0 &amp; \quad  |\hat{\theta} - \theta| \leq \epsilon \end{array}\right. = 1- \delta(\hat{\theta- \theta})$$`

Donde `\(delta\)` denota la *delta de Dirc*

--

`\(\longrightarrow\)`

`$$\int_{\Theta} ( 1 - \delta(\hat{\theta}- \theta))P(\theta|X_{(\underline{n})})d \theta = 1-\int_{\Theta}\delta(\hat{\theta}-\theta )P(\theta|X_{(\underline{n})})d \theta = 1 - P(\theta|X_{(\underline{n})})$$`

--

El problema de minimizar la función objetivo es equivalente a maximizar la densidad `\(\therefore\)`

--

$$ \quad \hat{\theta} = moda $$

---

class: inverse, center, middle
# 2

--
 
# Distribuciones Iniciales

---

# ¿Cómo funciona el proceso de inferencia Bayesiano?

--

- `\(P(\theta| X_{(\underline{n})}) \propto \mathbb{L}(\theta|X_{(\underline{n})})\cdot P(\theta)\)` es una manera conciliar nuestra **incertidumbre** sobre el parámetro de interés y la información que aportan los *datos*

--

Gráficamente tenemos lo siguiente

&lt;img src="Distribuciones_Iniciales_files/figure-html/prior-1.png" width="300" style="display: block; margin: auto;" /&gt;

---

# ¿Cómo funciona el proceso de inferencia Bayesiano?

- `\(P(\theta| X_{(\underline{n})}) \propto \mathbb{L}(\theta|X_{(\underline{n})})\cdot P(\theta)\)` es una manera conciliar nuestra **incertidumbre** sobre el parámetro de interés y la información que aportan los *datos*


Gráficamente tenemos lo siguiente

&lt;img src="Distribuciones_Iniciales_files/figure-html/prior1-1.png" width="300" style="display: block; margin: auto;" /&gt;

---

# ¿Cómo funciona el proceso de inferencia Bayesiana?

- `\(P(\theta| X_{(\underline{n})}) \propto \mathbb{L}(\theta|X_{(\underline{n})})\cdot P(\theta)\)` es una manera conciliar nuestra **incertidumbre** sobre el parámetro de interés y la información que aportan los *datos*


Gráficamente tenemos lo siguiente

&lt;img src="Distribuciones_Iniciales_files/figure-html/prior2-1.png" width="300" style="display: block; margin: auto;" /&gt;

---

class: center, middle
# La pregunta es: ¿Cómo determinamos una `\(P(\theta)\)` apropiada?

---

class: center, middle, inverse
# 2

--

# Distribuciones iniciales

---

# Distribuciones iniciales 

--

**R:** Depende...

--

- Recordemos que en el contexto del problema de inferencia `\(P(\theta)\)` cuantifca nuestra incertidumbre alrededor del parámetro de interés

--

- En este sentido las preguntas que debe responder la elección de alguna distribución en particular debe responder al menos las siguientes preguntas:

--

  1. ¿ `\(P(\theta)\)` es congruente con el espacio parametral `\(\Theta\)`?
  
--

  2. ¿Está centrada alrededor de algún valor?
  
--

  3. ¿Es simétrica?
  
--

  4. ¿Qué tanta variabilidad presenta? Ó en otras palabras ¿Cuál es mi nivel de certidumbre medido en términos (p.e.) de la desviación estándar?

--

- Dependiendo de estas características en particular de la distribución inicial y su interacción con la verosimilutud la distribución posterior tendrá diferentes caracterísitcas

---

# Distribuciones iniciales

--

- De acuerdo al grado de "certeza" del conocimiento *a priori* sobre el paramtero historicamente se les ha clasificado como distribuciones **informatvas** o **no informativas** (ó distribuciones **de referencia** en literatura más reciente)

--

&lt;img src="Distribuciones_Iniciales_files/figure-html/informativa2-1.png" width="300" style="display: block; margin: auto;" /&gt;

---

# Distribuciones iniciales

--

- De acuerdo al grado de "certeza" del conocimiento *a priori* sobre el paramtero historicamente se les ha clasificado como distribuciones **informatvas** o **no informativas** (ó distribuciones **de referencia** en literatura más reciente)

--

&lt;img src="Distribuciones_Iniciales_files/figure-html/informativa3-1.png" width="300" style="display: block; margin: auto;" /&gt;
---

# Distribuciones Iniciales de Referencia

--

- Uno de los motivos por los cuales Ronald A. Fisher, criticaba a la escuela Bayesiana era precisamente este elemento subjetivo intrínseco en la asignación de probabilidades *a priori* (Recordemos que la escuela frecuentista parte de postivismo de Augusto Comte por lo cual la fuente última de conocmiento es la experiencia)

--

- Además la asignación de probabilidades a través de la distribución uniforme es dificil de manipular al realizar la multiplcación de `\(\mathbb{L}(\theta |X_{(\underline{n})}) \cdot P(\theta)\)`

--

- Esto plantea un reto para la escuela bayesiana para encontrar un método de generar distribuciones de referencia. tales que se le diera "prioridad" a los datos y hacer la asignación *a priori* lo menos subjetiva posible

---

class: center, middle
# Pregunta: ¿Existe algún método para generar distribuciones de referencia que no sea la distribución uniforme?

---

# Distribución Inicial de Jeffreys

--

- Sir Harold Jeffreys, fue un matemático, estadístico, geofísico y astrónomo británico es uno de los padres de la estadística Bayesiana

--

- Jeffreys concluyó que una opción posible para generar distribuciones de referencia para cualquier modelo es la siguiente: `$$P(\theta) \propto I(\theta)^{\frac{1}{2}}$$`

--

- `\(I(\theta)\)` denota la **Información de Fisher**, es decir `$$I(\theta) \propto -\mathbb{E}[\frac{\partial^2}{\partial \theta^2}lnf(x|\theta)]$$`

---

# Resiliencia de la Verosimilitud

--

- A pesar de que parece que el investigador puede forzar cierta distribución *a- posteriori* a través de la distribución *a-priori*, existe un umbral a partir del cual la interacción con la verosimilitud se queda fija ante distribucione sinciales 'locas'

--

- Al análisis (que debe acompañar a cualquier análisis Bayesiano) del umbral dónde la distribución *a posteriori* deja de cambiar ante distribuciones iniciales "extremas" se le llama **resilencia de la verosismilitud** (Haro _ Peniche, 2020)

--

- Esto implica que hay un "seguro" sobre que tanto puede el investigador incorporar información *a-priori* al experimento

---


---
# Ejemplo Práctico

--

- Sea `\(x_i \sim Bernoulli(\theta)\)` y `\(\theta \sim U(0,1)\)`

--

**Pregunta:** ¿Cómo se distribuye `\(P(\theta | X_{(\underline{n})})\)`?

--

**R:** Sabemos que `$$P(\theta | X_{(\underline{n})}) \propto \mathbb{L}(\theta | X_{(\underline{n})}) P(\theta)$$`

--

Pero...

`$$\mathbb{L}(\theta|X_{(\underline{n})}) = \prod_{i = 1}^{n}f(x_i|\theta)$$`
--

Como `\(x_i \sim Bernoulli(\theta) \quad \Longrightarrow\)`

`$$\mathbb{L}(\theta|X_{(\underline{n})}) = \theta^{\sum_{I}x_i}(1-\theta)^{n - \sum_{I}x_i}$$`

---

# Ejemplo Práctico

`\(\Longrightarrow\)`

`$$p(\theta | x_{}(\underline{n})) \propto  \theta^{\sum_{I}x_i}(1-\theta)^{n- \sum_{n - \sum_{}I}} \frac{1}{\theta} \mathbb{1}_{(0,\theta)}\mathbb{1}_{(0,1)}$$`
--

**R:** No hay respuesta cerrada

---

# Ejemplo Práctico

--

- Sea `\(x_i \sim Bernoulli(\theta)\)` y `\(\theta \sim \beta(2,2)\)`

--

- `$$p(\theta | x_{}(\underline{n})) \propto  \theta^{\sum_{I}x_i}(1-\theta)^{n- \sum x_i} \frac{\theta^{\alpha - 1}(1- \theta)^{\beta - 1}}{B(\alpha, \beta)}$$`

--

- El operador `\(\propto\)` implica que todo lo que no se realacione con el parámetro `\(\theta\)` puede ser tratado como una constante

--

- `$$p(\theta | x_{}(\underline{n})) \propto  \theta^{\sum_{I}x_i + \alpha - 1}(1-\theta)^{n- \sum x_i + \beta - 1}$$`

--

-  `$$P(\theta | X_{(\underline{n}))} \propto  \beta(\theta | a = \sum_I x_i + \alpha, b = n - \sum x_i + \beta)$$`

--

¿Qué elecciones de `\(P(\theta)\)` y `\(x_i \sim f(x | \theta)\)` resultan en una familia parametrica fácil de manipular?

---


# ¿Qué sigue?

--

1. Familias conjugadas
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": true,
"self_contained": true,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>

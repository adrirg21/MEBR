---
title: "Métodos Estadísticos con R"
subtitle: "CODD"
author: "Gibrán Peniche"
date: "(versión 0.0.1) - 2020/06/25"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: ["xaringan-themer.css", "style.css"]
    nature:
      highlightStyle: github
      highlightLines: TRUE
      countIncrementalSlides: TRUE
      self_contained: TRUE
      ratio: '16:9'
    seal: false
editor_options: 
  chunk_output_type: console
---


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(eval = TRUE)
```

class: title-slide, middle

.pull-left[ 
# Métodos Estadísticos Bayesianos con R
## Familias Conjugadas
### Gibrán Peniche
### v. 0.0.1
### 2020-07-29
####  <i class="fab fa-github"></i> [jgpeniche](https://github.com/jgpeniche)
####  <i class="fab fa-twitter"></i> [PenicheGibran](https://twitter.com/PenicheGibran)
####  <i class="fab fa-google"></i> jgpeniche@gmail.com

]

.pull-right[

![](figs/logo_g.png)

]

---
# La sesión pasada...

--

- Mostramos diferentes estimadores puntuales para distintas funciones de pérdida

--

  1. La **media** corresponde a la función de *pérdida cuadrática*
  
--

  2. La **mediana** corresponde a la función de *pérdida absoluta*

--

  3. La **moda** corresponde a la función de *perdida (0,1)*

--

- Exploramos el concepto de distribución inicial

--

  1. Exploramos un poco la dinámica entre distribuciones informativas y no informativas en el proceso de inferencia bayesiano
  
--

  2. Enunciamos un método para generar distribuciones de referencia a través de *la inicial de Jeffreys* $P(\theta) \propto I(\theta)^{\frac{1}{2}}$. Dónde $I(\theta)$ denota la **Información de Fisher**
---

# Agenda

--

1. Familias Conjugadas

---

class: center, inverse, middle
# 1

--

# Familias Conjugadas

---

# $P(\theta | X_{(\underline{n})}) \propto \mathbb{L}(| X_{(\underline{n})}) P(\theta)$

--

Algunas observaciones...

--

El proceso de inferencia bayesiano es estrucuturalmente más complejo que el método frecuentista

--

- El producto $\mathbb{L}(\theta| X_{(\underline{n})})P(\theta)$ la mayoría de las veces no tiene forma analítica cerrada

--

- Los casos en los que este producto de **f.d.p.g** (funciones de desnidad de probabilidad generalizadas) define, bajo el operador de proporcionalidad ( $\propto$ ), un *kernel* conocido se les conoce como familias conjugadas  

--

- Este término fue acuñando por Raiffa y Schalifer en 1961

---

# Familias Conjugadas


Formalmente 

--

**Definición: ** Sea $X$ una variable aleatoria con f.d.p.g $f(X|\theta)$ con $\theta \quad \epsilon \quad \Theta$. La familia de distribuciones $\mathcal{F}=\{P(\theta) \}$ se dice que es cerrada bajo el muestreo $f(X | \theta)$, o conjugada del muestreo $f(X | \theta)$, si cuando la distribución a priori de $\theta$ pertenece a $\mathcal{F}$ la distribución a posteriori correspondiente también pertenece a $\mathcal{F}$ para toda muestra aleatoria $X_{(\underline{n})}$ de $X$

---

class: center, middle
# Sistema *Bernoulli* - $\beta$

---

# Sistema *Bernoulli* - $\beta$

--

De la clase pasada...

--

- Considere $X_{(\underline{n})}$ m.a. (Muestra aleatoria, esto implica una colección *i.i.d*) .,. (tal que) $x_i$ v.a (variable aleatoria) con f.d.p.g $Bernoulli(\theta)$ 

--

1. Determine la distribución inicial de Jeffreys y obtenga la distribución posterior

--

2. Determine la distribución posterior si $P(\theta) \sim \beta(\theta|a_1,b_1)$

--

```{r count1, echo = FALSE, message=FALSE, warning=FALSE}
countdown::countdown(minutes = 10L)

```

---

# Solución

--

1. Sabemos que $I(\theta) \propto - \mathbb{E}[\frac{\partial^2}{\partial \theta^2} \mathcal{l}(\theta)]$

--

Además $\mathcal{l}(\theta) = ln(\theta^{x}(1 - \theta)^{1-x}) = x ln(\theta) + (1 - x)ln(1 - \theta)$ 
--

$\Longrightarrow \quad \frac{\partial}{\partial \theta}l(\theta) = \frac{x}{\theta} - \frac{1 - x}{1 - \theta}$ 

--

$\Longrightarrow \quad \frac{\partial^2}{\partial \theta^2}l(\theta) = -\frac{x}{\theta^2} - \frac{1 - x}{(1 - \theta)^2}$

--

Finalmente $I(\theta)= -\mathbb{E}[-\frac{x}{\theta^2} - \frac{1 - x}{(1 - \theta)^2}] = \frac{\theta}{\theta^2}  \frac{1 - \theta}{(1 - \theta)^2} = \frac{1}{\theta(1 -\theta)}$

--

$\therefore P_{J}(\theta) \propto I(\theta)^{\frac{1}{2}} = \theta{^\frac{1}{2}}(1- \theta)^{\frac{1}{2}} \propto \beta(\theta | \alpha = 0.5, \beta = 0.5)$


--

Finalmente consideremos $\mathbb{L}(\theta|X_{(\underline{n})}) = f(X_{(\underline{n})}| \theta) = \prod_{i=1}^nf(x_i | \theta) = \prod_{i=1}^n \theta^{x_i}(1- \theta)^{1- x_i}$

$\theta^{\sum x_i}(1 - \theta)^{n- \sum x_i}$

---

# Solución

Luego entonces $\pi (\theta) \propto \mathbb{L}(\theta|X_{(\underline{n})}) P(\theta) \propto \theta^{\sum x_i}(1 - \theta)^{n- \sum x_i} \theta{^\frac{1}{2}}(1- \theta)^{\frac{1}{2}} =$

--

$\theta^{\sum x_i + \frac{1}{2}}(1 - \theta)^{n- \sum x_i + \frac{1}{2}} \sim \beta(\theta| a = \sum x_i + \frac{1}{2}, b =n- \sum x_i + \frac{1}{2})$

--

2. $\pi(\theta) \sim \beta(\theta| a = \sum x_i + a_1, b =n- \sum x_i + b_1$

---

class: center, middle
# Sistema $Exponencial - \Gamma$

---

# Sistema $Exponencial - \Gamma$

--

Considere $X_{(\underline{n})}$ m.a.  .,. $x_i \sim Exp(\theta)$ 

--

a. La inicial de Jeffreys está dada por $P_J \propto \frac{1}{\theta^{\frac{1}{2}}}$

--

b. Si $P(\theta) \sim \Gamma(\theta | \alpha, \beta) \quad \Longrightarrow \quad \pi(\theta) ~ \sim \Gamma(\theta | \alpha + n, \beta + n \overline{x})$

---

class: center, middle
# Sistema $Poisson - \Gamma$

---

# Sistema $Poisson - \Gamma$


Considere $X_{(\underline{n})}$ m.a.  .,. $x_i \sim Poisson(\theta)$ 

--

a. La inicial de Jeffreys está dada por $P_J(\theta) \propto \frac{1}{\theta^{\frac{1}{2}}}$

--

b. Si $P(\theta) \sim \Gamma(\theta | \alpha, \beta) \quad \Longrightarrow \quad \pi(\theta) ~ \sim \Gamma(\theta | \alpha + n \overline{x}, \beta + n)$

---

class: center, middle
# Sistema Normal (Varianza Conocida) - Normal para $\mu$

---

# Sistema Normal (Varianza Conocida) - Normal para $\mu$

Considere $X_{(\underline{n})}$ m.a.  .,. $x_i \sim N(\mu, \sigma^2)$ con $\sigma^2$ conocida 

--

a. La inicial de Jeffreys está dada por $P_J(\mu) \propto 1$

--

b. Si $P(\mu) \sim N(\mu | \mu_0, \sigma^2_0) \quad \Longrightarrow \quad \pi(\mu) ~ \sim N(\mu | \mu_n = \frac{\sigma^2_0 n \overline{x}}{n \sigma^2_0 + \sigma^2} + \frac{\sigma^2 \mu_0}{n\sigma_0^2 + \sigma^2}, \beta + n)$

--

Este estimador es interesante porque es una combinación lineal convexa entre $\mu_0$ y $\overline{x}$

--

Un resultado interesante es que si la el tamaño de la muestra crece, en límite converge al estimador frecuentista $\overline{x}$ ( $n \longrightarrow \infty$ )

---

class: center, middle
# Sistema Normal (media conocida) - $\Gamma^{-1}$

---

# Sistema Normal (media conocida) - $\Gamma$

Considere $X_{(\underline{n})}$ m.a.  .,. $x_i \sim N(\mu, \sigma^2)$ con $\sigma^2$ conocida 

--

a. La inicial de Jeffreys está dada por $P_J(\mu) \propto \frac{1}{\sigma}$

--

b. Si $P(\sigma) \sim \Gamma(\tau |\alpha, \beta) \quad \Longrightarrow \quad \pi(\sigma^2) ~ \sim \Gamma(\tau | \alpha + \frac{1}{2}, \beta + \frac{1}{2} \sum (x_i - \mu))$

--

Donde $\tau = \frac{1}{2}$

---

class: center, middle
# Sistema Normal - $\mu| \tau \sim N(\mu_0, \kappa \tau)$ - $\tau \sim \Gamma(\alpha, \beta)$

---

# Sistema Normal - $\mu| \tau \sim N(\mu_0, \kappa \tau)$ - $\tau \sim \Gamma(\alpha, \beta)$

--

Considere $X_{(\underline{n})}$ m.a.  .,. $x_i \sim N(\mu, \sigma^2)$ con $\sigma^2$ conocida 

--

a. La inicial de Jeffreys está dada por $P_J(\mu, \sigma^2) = \sqrt{det|I(\mu, \sigma^2)|)} \propto \frac{1}{\sigma^3}$

--

b. Si $P(\mu | \tau) \sim N(\mu |\mu_0, \kappa \tau) \quad y \quad P(\tau ) \sim \Gamma(\tau, | \alpha, \beta)$ 

--

$\Longrightarrow \quad \pi(\tau) ~ \sim \Gamma(\tau | \alpha + \frac{n}{2}, \beta + \frac{1}{2} \sum (x_i - \overline{x})^2 + \frac{n \kappa}{2(n + \kappa)}(\overline{x}- \mu_0)^2)$

--

$\Longrightarrow \pi(\mu) \sim N(\mu | \frac{n \tau \overline{x}}{n \tau + \kappa \tau}+ \frac{\kappa \tau}{n \tau + \kappa \tau}, n \tau + \kappa \tau)$


---

# Algunos otros sistemas ...

--

1. Uniforme - Pareto 

--

2. Multinomial - Dirichlet

--

3. Mezcla de Normales - Proceso Dirichlet


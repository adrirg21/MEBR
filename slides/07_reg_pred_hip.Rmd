---
title: "Métodos Estadísticos con R"
subtitle: "CODD"
author: "Gibrán Peniche"
date: "(versión 0.0.1) - 2020/06/25"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: ["xaringan-themer.css", "style.css"]
    nature:
      highlightStyle: github
      highlightLines: TRUE
      countIncrementalSlides: TRUE
      self_contained: TRUE
      ratio: '16:9'
    seal: false
editor_options: 
  chunk_output_type: console
---


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(eval = TRUE)
```

class: title-slide, middle

.pull-left[ 
# Métodos Estadísticos Bayesianos con R
## Estimación por regionaes, predicción y pruebas de hipótesis
### Gibrán Peniche
### v. 0.0.1
### 2020-10-01
####  <i class="fab fa-github"></i> [jgpeniche](https://github.com/jgpeniche)
####  <i class="fab fa-twitter"></i> [PenicheGibran](https://twitter.com/PenicheGibran)
####  <i class="fab fa-google"></i> jgpeniche@gmail.com

]

.pull-right[

![](figs/logo_g.png)

]

---
# La sesión pasada...

--

- Enunciamos una manera de introducir distribuciones iniciales de referencia a través de la inicial de Jeffreys

--

- Definimos algunos sistemas conjugados

---

# Agenda

--

1. Estimación por regiones

--

2. Predicción

--

3. Pruebas de hipóteis

---

class: center, inverse, middle
# 1

--

# Estimación por regiones

---

# Estimación por regiones
--

Desde el punto de vista frecuentista utilizamos el método pivotal...

--

Por ejemplo: $\frac{x - \overline{x}}{\hat{\sigma}} \sim t_{n-1}$

Dónde $\hat{\sigma} \sim \mathfrak{X}^2_{n-1}$ y $x_i \sim N(\mu, \sigma^2)$ ambos desconocidos

--

A partir del cual construimos un intervalo de confianza para la cantidad desconocida $\mu$, donde con un nivel de **confianza** del $(1 - \alpha)$% la cantidad desconocida $\mu \quad \epsilon \quad [\overline{x} \pm t_{(1 - \frac{\alpha}{2})} \cdot \hat{\sigma}]$ para una muestra **fija** el $(1 - \alpha)$% de las veces

--

Algunos problemas (para variar)...  

--

  1. El método pivotal requiere de una distribución **conocida** para el cociente

--

  2. Por lo anterior, generalmente se recurre al teorema del límite central y genera comportamientos extraños
  
---

# Estimación por regiones

Desde el punto de vista bayesiano...

--

Hacemos inferencia sobre $\theta$ a partir de la distribución posterior $\pi(\theta) = p(\theta|X_{(\underline{n})})$ y elegimos un estimador puntual $\hat{\theta}$ a partir de cierta función de pérdida $\mathfrak{L}(\hat{\theta}, \theta)$

--

Por construcción las afirmaciones que hacemos sobre $\theta$ son de naturaleza **probabilistica** y son carcterizadas por $\pi(\theta)$

--

$\longrightarrow$ El intervalo de **credibilidad** para $\theta$ al nivel $(1-\alpha)$ está dado por $q_{\pi,a}$ y $q_{\pi,b}$ .,. $p_{\pi}(a < \theta < b) = 1 - \alpha$. Donde $q$ representa un **cuantil** de cierto orden de $\pi(\theta)$

--

Noten como a diferencia de los intervalos de *confianza*. los intervalos de *credibilidad* no tienen por que ser simétricos (como tampoco tiene porque serlo la distribución posterior)

---

class: inverse, center, middle
# 2

--

# Predicción

---

# Predicción

Tal vez la utilidad prinicpal de la inferencia sea resolver el problema de predicción de una cantidad $x^*$ que **aún no se ha observado**

--

Desde el punto de vista frecuentista se introduce $\hat{\theta}$ en $p(x | \theta)$ y el *intervalo de pronóstico* se construye por el método pivotal

--

Por ejemplo: $\frac{x - x^*}{\hat{\sigma}}$ para el caso que discutimos anteriormente

---

# Predicción

Desde el punto de vista Bayesiano tenemos otra herramienta. la distribucción **posterior predictiva**

--

La idea que rodea este concepto es que queremos una descripción de la incertidumbre asociada a $x^*$ dados los datos $X_{(\underline{n})}$ y la incertidumbre asociada a $\theta$, esto es $p(x^* | X_{(\underline{n})})$

--

$$p(x^* | X_{(\underline{n})}) = \int_{\Theta} p(x^* | \theta ) \pi(\theta) d \theta$$
-- 

En cristiano, es un promedio sobre $p(x^*)$ dada la incertudumbre *a posteriori* de $\theta$ (algo como un $\mathbb{E}[g(x)]$)

---

# Posteriores predictivas conjugadas

--

1. $Poisson - \Gamma$ $\Rightarrow$ Binomial Negativa

--

2. $Bernoulli - \beta$ $\rightarrow$ Distribución - Beta Bernoulli

--

3. $Normal - Normal$ $\rightarrow$ Normal (sigma conocida)

--

Entre otras ...

---

class: inverse, center, middle
# 3

--

# Pruebas de hipótesis

---

# Pruebas de hipótesis

--

Otro de los grandes problemas de la inferencia es contrastar valores alternativos de $\theta$ para un modelo

--

Desde el punto de vista frecuentista usamos el *Lema de Neyman-Pearson* del **conciente de verosimilitudes generalizado** (que tiene que ver con la función potencia de la prueba $\alpha$(x))  los errores Tipo I y Tipo II. Que involucra una demostración complejisima y concluye que el examen debe hacerse en el **supremo máximo verosímil** bajo cada hipótesis.

--

Desde el punto de vista bayesiano simplemte operamos $\pi(\theta)$ en términos de **probabilidad** (Fun fact también se puede hacer *a priori*)

---

class: center, middle
# ¿Qué pasa cuando no estamos en el caso conjugado?

---

# Inferencia Bayesiana computacional

--

1. Cadenas de Markov

--

2. Simulación estocástica Markov Chain Monte Carlo (MCMC)

--

3. Stan

